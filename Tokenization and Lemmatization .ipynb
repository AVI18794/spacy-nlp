{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SPACY.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPINxKlwtmkrocvpbv8l64N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AVI18794/spacy-nlp/blob/main/Tokenization%20and%20Lemmatization%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZoqBp1Bwyl0"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNyz6yHfw2xy"
      },
      "source": [
        "# !python -m spacy download en_core_web_md"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxZ41hfhw9kg"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46y8Wdwfxrau"
      },
      "source": [
        "doc = nlp(\"I went there\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Qc4J2YNxut1",
        "outputId": "9172aa67-2a37-46bd-c02c-d1bf9efa92c5"
      },
      "source": [
        "doc"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "I went there"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xfx97NHyWg5",
        "outputId": "5d0cd2bc-528d-48f3-c6a2-a80bd45413da"
      },
      "source": [
        "doc[1].tag"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17109001835818727656"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98gbcGBxydzT",
        "outputId": "01640f3f-a136-47fd-aabe-b16e0ea33deb"
      },
      "source": [
        "len(doc)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KlelOxAyqfZ",
        "outputId": "dbd9147c-d100-48d3-cbc8-122ae0c282aa"
      },
      "source": [
        "for i,text in enumerate(doc):\n",
        "  print(doc[i])\n",
        "  print(doc[i].tag)\n",
        "  print(doc[i].dep)\n",
        "  # print(doc[i].head)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I\n",
            "13656873538139661788\n",
            "429\n",
            "went\n",
            "17109001835818727656\n",
            "8206900633647566924\n",
            "there\n",
            "164681854541413346\n",
            "400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFYjn3KRyu4f",
        "outputId": "098394fd-6e9f-4ad0-f848-c39edfc974b0"
      },
      "source": [
        "#Tokenization\n",
        "#Tokens are the smallest individual unit of a sentence.\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc = nlp(\"I own a ginger Cat.\")\n",
        "print([token.text for token in doc])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'own', 'a', 'ginger', 'Cat', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gouAe9UZzbdf",
        "outputId": "5a75cf18-a4e5-41be-c994-087cbf184129"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc = nlp(\"It's been a crazy week!!!\")\n",
        "print([token.text for token in doc])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['It', \"'s\", 'been', 'a', 'crazy', 'week', '!', '!', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5KiwDrh6ory"
      },
      "source": [
        "# Customizing the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_nW-Dl73Zg5",
        "outputId": "5c0f8265-1c3c-4ed5-a6d2-cbd69d49b465"
      },
      "source": [
        "import spacy\n",
        "from spacy.symbols import ORTH\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "doc = nlp(\"Lemme That\")\n",
        "print([w.text for w in doc])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Lemme', 'That']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywpFw0b36ltM"
      },
      "source": [
        "special_case = [{ORTH:'Lem'},{ORTH:'me'}]\n",
        "nlp.tokenizer.add_special_case(\"Lemme\",special_case)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_hZtLKZ7CCk",
        "outputId": "bf280f43-a6bf-429b-a33b-4fcddcc69981"
      },
      "source": [
        "print([w.text for w in nlp(\"Lemme that\")])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Lem', 'me', 'that']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvsMtn6D8OY7"
      },
      "source": [
        "# Debugging the tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD6kQEb68OCl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0se7amc7OOC",
        "outputId": "c3a5e26d-f287-4c5b-9993-429388ca1360",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "text = \"Let's go!\"\n",
        "doc = nlp(text)\n",
        "token_explain = nlp.tokenizer.explain(text)\n",
        "for t in token_explain:\n",
        "  print(t[1],\"\\t\",t[0])\n",
        "  "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Let \t SPECIAL-1\n",
            "'s \t SPECIAL-2\n",
            "go \t TOKEN\n",
            "! \t SUFFIX\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEtcQ1q69R4M"
      },
      "source": [
        "# Sentence Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmjDVX5Y8w6g",
        "outputId": "ceeb6712-b156-4024-8501-3f0b27bb0fbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "text = \"I flied to N.Y. yesterday.It was around 6 pm.\"\n",
        "doc = nlp(text)\n",
        "for sent in doc.sents:\n",
        "  print(sent.text)\n",
        "  "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I flied to N.Y. yesterday.\n",
            "It was around 6 pm.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN_m321_9_Ej"
      },
      "source": [
        "# Lemmatization\n",
        "\n",
        "A lemma is the **base form** of a **token**. You can think of a lemma as the form\n",
        "in which the token appears in a dictionary. For instance, the lemma of\n",
        "eating is eat; the lemma of eats is eat; ate similarly maps to eat.\n",
        "**Lemmatization is the process of reducing the word forms to their lemmas.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRwjudDt9sKK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}